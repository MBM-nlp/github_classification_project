{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metavers NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: **Mijail Mariano**, **Meredith Wang**, **Brad Gauvin**\n",
    "\n",
    "August 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:02:28.684839Z",
     "start_time": "2022-09-06T21:02:28.663704Z"
    }
   },
   "source": [
    "<a href=\"#\"><img align=\"left\" alt=\"Python\" src=\"https://img.shields.io/badge/Python-013243.svg?logo=python&logoColor=white\"></a>\n",
    "<a href=\"#\"><img align=\"left\" alt=\"Pandas\" src=\"https://img.shields.io/badge/Pandas-150458.svg?logo=pandas&logoColor=white\"></a>\n",
    "<a href=\"#\"><img align=\"left\" alt=\"NumPy\" src=\"https://img.shields.io/badge/Numpy-2a4d69.svg?logo=numpy&logoColor=white\"></a>\n",
    "<a href=\"#\"><img align=\"left\" alt=\"Matplotlib\" src=\"https://img.shields.io/badge/Matplotlib-8DF9C1.svg?logo=matplotlib&logoColor=white\"></a>\n",
    "<a href=\"#\"><img align=\"left\" alt=\"seaborn\" src=\"https://img.shields.io/badge/seaborn-65A9A8.svg?logo=pandas&logoColor=white\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">------------ üèòÔ∏è Project Goal ------------</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:03:10.199449Z",
     "start_time": "2022-09-06T21:03:10.179783Z"
    }
   },
   "source": [
    "Use README.md text to learn the primary programming language used in \"Metaverse\" related repos and associated topics. Design an NLP predictive model that accurately identifies the *[programming language]* * used in \"metaverse\" related repos. \n",
    "\n",
    "*The predicted programming language used in this analysis is the primary (in overall repo percentage) that is found in the GitHub repository searched.* *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:03:36.730708Z",
     "start_time": "2022-09-06T21:03:36.702527Z"
    }
   },
   "source": [
    "<h1 align=\"center\">------------ üßÆ Acquire Data ------------</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition\n",
    "‚ñ™Ô∏è This is accomplished via the python script named ‚Äúacquire.py‚Äù. The script will use credentials (stored in env.py) to collect data from GitHub.com in various ways\n",
    "\n",
    "‚ñ™Ô∏è First, scrape \"URLs\", or Repository names, so that the subsequent acquisition function will be able to seek out those repositories. Store the names of the repositories in a `metaverse.csv` file.\n",
    "\n",
    "‚ñ™Ô∏è Once the list of repositories is collected, use functions from the acquire script to collect the following information from those repositories, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:23:09.492859Z",
     "start_time": "2022-09-06T22:23:09.455303Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# notebook dependencies \n",
    "import os # used in caching\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# regular expression import\n",
    "import re\n",
    "\n",
    "# JSON import\n",
    "import json\n",
    "\n",
    "# importing BeautifulSoup for parsing HTML/XTML\n",
    "from bs4 import BeautifulSoup as BSoup\n",
    "\n",
    "# request module for connecting to APIs\n",
    "from requests import get\n",
    "\n",
    "# text prepare modules\n",
    "import acquire\n",
    "import prepare\n",
    "\n",
    "# uni-code library\n",
    "import unicodedata\n",
    "\n",
    "# natural language toolkit library/modules\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:11.480046Z",
     "start_time": "2022-09-06T21:13:11.447934Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pulling in the acquire df\n",
    "\n",
    "df = pd.read_csv(\"metaverse.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:11.488725Z",
     "start_time": "2022-09-06T21:13:11.480951Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# df info\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:11.492496Z",
     "start_time": "2022-09-06T21:13:11.490007Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# specifying the column dtypes\n",
    "\n",
    "df = df[[\"repo\", \"language\", \"readme_contents\"]].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation - Data Cleaning üßπ \n",
    "‚ñ™Ô∏è **Missing Values (NaN):**\n",
    "    \n",
    "- All the null language/readme values are <span style=\"color: blue\"> **converted to `text`** </span>\n",
    "\n",
    "‚ñ™Ô∏è **Data Conversion**\n",
    "- Convert `readme` to `leaned`, `stemmed` and `lemmatized`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:15.528819Z",
     "start_time": "2022-09-06T21:13:11.493268Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# viewing the clean/tokenize/stem/lemmatization process\n",
    "\n",
    "df[\"clean\"] = df[\"readme_contents\"].apply(prepare.basic_clean).apply(prepare.tokenize).apply(prepare.remove_stopwords, include_words = [\"metaverse\", \"Metaverse\", \"meta-verse\", \"Meta-verse\", \"meta verse\", \"Meta Verse\", \"Meta verse\"])\n",
    "df[\"stemmed\"] = df[\"clean\"].apply(prepare.porter_stem)\n",
    "df[\"lemmatized\"] = df[\"clean\"].apply(prepare.lemmatize)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:15.558493Z",
     "start_time": "2022-09-06T21:13:15.529679Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Cleaned Data function\n",
    "df = pd.read_csv(\"metaverse.csv\")\n",
    "\n",
    "df = prepare.clean_data_objects(df)\n",
    "df.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.746545Z",
     "start_time": "2022-09-06T21:13:15.559413Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# using the mass_text_clean function\n",
    "\n",
    "df[\"readme_contents\"] = df[\"readme_contents\"].apply(prepare.mass_text_clean)\n",
    "df.head() # check outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.761279Z",
     "start_time": "2022-09-06T21:13:16.747312Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Add word count\n",
    "df['word_count']=df.readme_contents.str.split().str.len()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to categorize some of the categories.\n",
    "- Multiple \"C\" languages\n",
    "- Multiple names for similar languages (Jupyternotebook/python, Powershell/Shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.785367Z",
     "start_time": "2022-09-06T21:13:16.770253Z"
    }
   },
   "outputs": [],
   "source": [
    "df=prepare.update_languages(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">------------ üë©üèª‚Äçüíª Exploratory Analysis ------------</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the following questions using data visualization and statistical testing:\n",
    "\n",
    "1. What are the most common words in the README files by language?\n",
    "2. Does the length of the README file vary by language?\n",
    "3. Are bigrams from the README useful for determining which language the repository belongs to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.805356Z",
     "start_time": "2022-09-06T21:13:16.793355Z"
    }
   },
   "outputs": [],
   "source": [
    "train, validate, test = prepare.train_validate_test_split(df, 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.817788Z",
     "start_time": "2022-09-06T21:13:16.813085Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:18:28.484668Z",
     "start_time": "2022-09-06T21:18:28.473457Z"
    }
   },
   "source": [
    "### Question: What are the most common words in the 'README's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.856389Z",
     "start_time": "2022-09-06T21:13:16.819107Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Isolate words by type/frequency\n",
    "java_words = (' '.join(train.readme_contents[train.language == 'Java'])).split()\n",
    "python_words = (' '.join(train.readme_contents[train.language == 'Python'])).split()\n",
    "c_words = (' '.join(train.readme_contents[train.language == 'C'])).split()\n",
    "html_words = (' '.join(train.readme_contents[train.language == 'HTML'])).split()\n",
    "type_words = (' '.join(train.readme_contents[train.language == 'TypeScript'])).split()\n",
    "other_words = (' '.join(train.readme_contents[train.language == 'other'])).split()\n",
    "solidity_words = (' '.join(train.readme_contents[train.language == 'Solidity'])).split()\n",
    "css_words = (' '.join(train.readme_contents[train.language == 'CSS'])).split()\n",
    "rust_words = (' '.join(train.readme_contents[train.language == 'Rust'])).split()\n",
    "go_words = (' '.join(train.readme_contents[train.language == 'Go'])).split()\n",
    "all_words = (' '.join(train.readme_contents)).split()\n",
    "\n",
    "java_freq = pd.Series(java_words).value_counts()\n",
    "python_freq = pd.Series(python_words).value_counts()\n",
    "c_freq = pd.Series(c_words).value_counts()\n",
    "html_freq = pd.Series(html_words).value_counts()\n",
    "type_freq = pd.Series(type_words).value_counts()\n",
    "other_freq = pd.Series(other_words).value_counts()\n",
    "solidity_freq = pd.Series(solidity_words).value_counts()\n",
    "css_freq = pd.Series(css_words).value_counts()\n",
    "rust_freq = pd.Series(rust_words).value_counts()\n",
    "go_freq = pd.Series(go_words).value_counts()\n",
    "all_freq = pd.Series(all_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.878380Z",
     "start_time": "2022-09-06T21:13:16.857107Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# concatonate words for count\n",
    "word_counts = (pd.concat([all_freq, java_freq, python_freq, c_freq, html_freq, type_freq, other_freq, solidity_freq, css_freq, rust_freq, go_freq], axis=1, sort=True)\n",
    "              .set_axis(['all','Java', 'Python', 'C', 'HTML','TypeScript','other','Solidity','CSS','Rust', 'Go'], axis=1, inplace=False)\n",
    "              .fillna(0)\n",
    "              .apply(lambda s: s.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.884155Z",
     "start_time": "2022-09-06T21:13:16.879230Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Pring word counts\n",
    "print('java words: ' + str(len(java_words)))\n",
    "print('python words: ' + str(len(python_words)))\n",
    "print('c words: ' + str(len(c_words)))\n",
    "print('html words: ' + str(len(html_words)))\n",
    "print('type words: ' + str(len(type_words)))\n",
    "print('other words: ' + str(len(other_words)))\n",
    "print('solidity words: ' + str(len(solidity_words)))\n",
    "print('css words: ' + str(len(css_words)))\n",
    "print('rust words: ' + str(len(rust_words)))\n",
    "print('go words: ' + str(len(go_words)))\n",
    "print('all words: ' + str(len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Question: What are the most common words in the 'README's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:16.892173Z",
     "start_time": "2022-09-06T21:13:16.884925Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display Overall top 20 words\n",
    "word_counts.sort_values(by='all', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:25:10.688184Z",
     "start_time": "2022-09-06T21:25:10.489473Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# All content\n",
    "all_content = ' '.join(train.readme_contents)\n",
    "# All freq\n",
    "all_freq = pd.DataFrame(pd.Series((all_content.split())).value_counts().head(20), columns = ['frequency'])\n",
    "# Visualize most frequent words\n",
    "plt.figure(figsize = (16,8))\n",
    "plt.xlabel('Words', fontsize = 15)\n",
    "plt.ylabel('Frequency', fontsize = 15)\n",
    "plt.title('Top 20 Most Common Words in Metaverse READMEs', fontsize = 20)\n",
    "sns.barplot(x=all_freq.index, y=all_freq.frequency, palette ='Purples_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:20:21.671467Z",
     "start_time": "2022-09-06T21:20:21.647170Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define language labels\n",
    "labels = pd.concat([train.language.value_counts(),\n",
    "                    train.language.value_counts(normalize=True)], axis=1)\n",
    "labels.columns = ['n', 'percent']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:20:24.534494Z",
     "start_time": "2022-09-06T21:20:24.362016Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Pie Graph for languages\n",
    "plt.figure(figsize=(16,12))\n",
    "mylabels = ['text', 'Java', 'C', 'TypeScript', 'HTML', 'other', 'Python', 'Solidity',\n",
    "       'CSS', 'Rust', 'Go']\n",
    "mycolors = ['#484553', '#3c1361', '#663a82', '#89609e', '#a86cc1', '#c377e0', '#cd8de5', '#d5a6e6', '#dfc0eb', '#eddbf4', '#f7f0fa']\n",
    "textprops = {\"fontsize\":15}\n",
    "plt.pie(labels.percent, labels = mylabels, colors = mycolors, textprops=textprops, autopct='%.1f%%')\n",
    "plt.legend()\n",
    "plt.title('Overall Language Distribution',fontsize=18)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:41:01.075359Z",
     "start_time": "2022-09-06T21:41:01.056239Z"
    }
   },
   "source": [
    "<h3><div class=\"alert alert-info\">Takeaway: '0', 'app', 'doc', 'create', 'img', 'next', 'project', 'run', etc. are the most common words.</div></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Question: Does the length of the README vary by programming language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div class=\"alert alert-info\"> Takeaway:\n",
    "    \n",
    "‚óæ Yes, READMEs length vary by programming language.\n",
    "    \n",
    "‚óæ READMEs that have the longest length on average are Rust, text, Python, typescript, and other.\n",
    "    \n",
    "‚óæ READMEs that have the shortest length on average are HTML, CSS, GO, Java, and Solidity.\n",
    "</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.070727Z",
     "start_time": "2022-09-06T21:13:22.061166Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.078240Z",
     "start_time": "2022-09-06T21:13:22.073287Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Find the readme length by programming language\n",
    "train.groupby('language').word_count.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.085014Z",
     "start_time": "2022-09-06T21:13:22.080002Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Bar plot function for Vis\n",
    "def bar_plot(train, target, readme_length):\n",
    "    '''\n",
    "    This function is to create a bar plot. Take in dataframe, a target and a feature\n",
    "    '''\n",
    "    train.groupby(target)[readme_length].mean().plot.bar()\n",
    "    plt.title('Mean Readme Lengths by Language')\n",
    "    plt.ylabel('Readme Length')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.176248Z",
     "start_time": "2022-09-06T21:13:22.089965Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# A visualization of the above outcome\n",
    "bar_plot(train, 'language', 'word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.179838Z",
     "start_time": "2022-09-06T21:13:22.177205Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import stats\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.189403Z",
     "start_time": "2022-09-06T21:13:22.182471Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def stat_test(train, readme_length):\n",
    "    '''\n",
    "    Perform 1 sample t-test comparing mean length of original\n",
    "    README file per language to the overall average length (all languages)\n",
    "    set the significance level to 0.05\n",
    "    '''\n",
    "    alpha = 0.05\n",
    "    overall_mean_length_readme = train[readme_length].mean()\n",
    "    for l in train.language.unique():\n",
    "        sample = train[train.language == l]\n",
    "        t,p = stats.ttest_1samp(sample[readme_length], overall_mean_length_readme)\n",
    "        print(l, round(t,5), p<alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.199680Z",
     "start_time": "2022-09-06T21:13:22.190668Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Perform 1 sample t-test comparing mean length of stemmed README file per language to the overall average length (all languages)\n",
    "# Set significance level to 0.05\n",
    "stat_test(train, 'word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:45:31.878396Z",
     "start_time": "2022-09-06T21:45:31.851632Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Concatenate readme content together for each language\n",
    "text = ' '.join(train[train.language == 'text'].readme_contents)\n",
    "java = ' '.join(train[train.language == 'Java'].readme_contents)\n",
    "c = ' '.join(train[train.language == 'C'].readme_contents)\n",
    "typescript = ' '.join(train[train.language == 'TypeScript'].readme_contents)\n",
    "html = ' '.join(train[train.language == 'HTML'].readme_contents)\n",
    "other = ' '.join(train[train.language == 'other'].readme_contents)\n",
    "python = ' '.join(train[train.language == 'Python'].readme_contents)\n",
    "solidity = ' '.join(train[train.language == 'Solidity'].readme_contents)\n",
    "css = ' '.join(train[train.language == 'CSS'].readme_contents)\n",
    "rust = ' '.join(train[train.language == 'Rust'].readme_contents)\n",
    "go = ' '.join(train[train.language == 'Go'].readme_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:45:34.265542Z",
     "start_time": "2022-09-06T21:45:34.129925Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Putting all language content in a dataframe\n",
    "language = pd.DataFrame((text,java,c,typescript,html,other,python,solidity, css,rust,go), index =['text', 'java', 'c', 'typescript', 'html', 'other', 'python','solidity', 'css', 'rust', 'go'], columns = ['content'])\n",
    "# Create length column that represents the total length of the content\n",
    "language['length'] = language.content.str.len()\n",
    "# Create count column -> the number of repo of each language\n",
    "language['count'] = [177, 159, 57, 40, 40, 22, 18, 15, 13, 9, 8]\n",
    "# Create avg length column represents the avg lenth of readme countent per language\n",
    "language['avg_length'] = language['length']/language['count']\n",
    "# Sort dataframe by avg length to show the longest readmes\n",
    "language = language.sort_values(by ='avg_length', ascending = False)\n",
    "# Visualize most frequent words\n",
    "plt.figure(figsize = (18,8))\n",
    "plt.xlabel('Programming Language', fontsize = 15)\n",
    "plt.ylabel('Avg README Length', fontsize = 15)\n",
    "plt.title('Average README Length by Programming Language', fontsize = 20)\n",
    "sns.barplot(x=language.index, y=language['avg_length'], palette ='Purples_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:25:32.182822Z",
     "start_time": "2022-09-06T22:25:32.173902Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Box plot\n",
    "def box_plot(train, target, word_count):\n",
    "    '''\n",
    "    Create a boxplot to represent the distributions of the README lengths. Take in a target and a feature\n",
    "    '''\n",
    "    sns.boxplot(x= target,y = word_count, data= train, palette='Purples_r')\n",
    "    plt.title('DIstribution of README length')\n",
    "    plt.ylim(0, 3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:25:35.651689Z",
     "start_time": "2022-09-06T22:25:35.467421Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# A boxplot to represent the distributions of the README lengths\n",
    "box_plot(train, 'language', 'word_count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "- The readme length varies by programming language\n",
    "- Rust has the longest average readme length\n",
    "- There are some outliers with readme lengths over 1,000\n",
    "- The length of the readme is statistically different from the overall mean for CSS and HTML. Other Languages did not show a statistically significant difference likely due to the high standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div class=\"alert alert-info\"> Takeaway:\n",
    "    \n",
    "‚óæ Yes, READMEs length vary by programming language.\n",
    "    \n",
    "‚óæ READMEs that have the longest length on average are Rust, text, Python, typescript, and other.\n",
    "    \n",
    "‚óæ READMEs that have the shortest length on average are HTML, CSS, GO, Java, and Solidity.\n",
    "</div></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Do different programming languages use a different number of unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.465936Z",
     "start_time": "2022-09-06T21:13:22.459486Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# unique counts function\n",
    "def unique_counts(word_counts):\n",
    "    '''\n",
    "    This function takes our word_counts dataframe and finds the number of uniques to each language and returns them as their own dataframes. \n",
    "    '''\n",
    "    # Adding a column for each language with 1 or 0 (str) to represent if it is unique to that language\n",
    "    word_counts['unique_p'] = np.where(word_counts['all'] == word_counts['Python'], '1', '0')\n",
    "    word_counts['unique_j'] = np.where(word_counts['all'] == word_counts['Java'], '1', '0')\n",
    "    word_counts['unique_c'] = np.where(word_counts['all'] == word_counts['C'], '1', '0')\n",
    "    word_counts['unique_r'] = np.where(word_counts['all'] == word_counts['Rust'], '1', '0')\n",
    "    word_counts['unique_h'] = np.where(word_counts['all'] == word_counts['HTML'], '1', '0')\n",
    "    word_counts['unique_t'] = np.where(word_counts['all'] == word_counts['TypeScript'], '1', '0')\n",
    "    word_counts['unique_g'] = np.where(word_counts['all'] == word_counts['Go'], '1', '0')\n",
    "    word_counts['unique_o'] = np.where(word_counts['all'] == word_counts['other'], '1', '0')\n",
    "    word_counts['unique_s'] = np.where(word_counts['all'] == word_counts['Solidity'], '1', '0')\n",
    "    word_counts['unique_css'] = np.where(word_counts['all'] == word_counts['CSS'], '1', '0')\n",
    "    # Getting separate df's for these unique words\n",
    "    unique_p = word_counts[['Python']][word_counts.unique_p == '1']\n",
    "    unique_j = word_counts[['Java']][word_counts.unique_j == '1']\n",
    "    unique_c = word_counts[['C']][word_counts.unique_c == '1']\n",
    "    unique_r = word_counts[['Rust']][word_counts.unique_p == '1']\n",
    "    unique_h = word_counts[['HTML']][word_counts.unique_j == '1']\n",
    "    unique_t = word_counts[['TypeScript']][word_counts.unique_c == '1']\n",
    "    unique_g = word_counts[['Go']][word_counts.unique_p == '1']\n",
    "    unique_o = word_counts[['other']][word_counts.unique_j == '1']\n",
    "    unique_s = word_counts[['Solidity']][word_counts.unique_c == '1']\n",
    "    unique_css = word_counts[['CSS']][word_counts.unique_c == '1']\n",
    "    # returning the three dataframes\n",
    "    return unique_p, unique_j, unique_c, unique_r, unique_h, unique_t, unique_g, unique_o, unique_s, unique_css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.485691Z",
     "start_time": "2022-09-06T21:13:22.469193Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Using our unique_counts() function to get our unique dataframes. Takes our previously acquired word_counts df.\n",
    "unique_p, unique_j, unique_c, unique_r, unique_h, unique_t, unique_g, unique_o, unique_s, unique_css = unique_counts(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:13:22.490882Z",
     "start_time": "2022-09-06T21:13:22.486988Z"
    }
   },
   "outputs": [],
   "source": [
    "len(unique_p), len(unique_j), len(unique_c),len(unique_r), len(unique_h), len(unique_t), len(unique_g), len(unique_o), len(unique_s), len(unique_css) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:48:48.455426Z",
     "start_time": "2022-09-06T21:48:48.411510Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Text freq\n",
    "text_freq = pd.DataFrame(pd.Series(text.split()).value_counts().head(20), columns = ['frequency'])\n",
    "text_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:49:28.665621Z",
     "start_time": "2022-09-06T21:49:28.505584Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Visualize most frequent words for text repos\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.xlabel('Words', fontsize = 15)\n",
    "plt.ylabel('Frequency', fontsize = 15)\n",
    "plt.title(\"Top 20 Most Common Words for Programming Language 'text'\", fontsize = 20)\n",
    "sns.barplot(x=text_freq.index, y=text_freq.frequency, palette ='Purples_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:50:20.554282Z",
     "start_time": "2022-09-06T21:50:20.363955Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Visual for C Languages\n",
    "# Storing frequency into dataframe\n",
    "c_freq = pd.DataFrame(pd.Series(c.split()).value_counts().head(20), columns = ['frequency'])\n",
    "# Visualize most frequent words for text repos\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.xlabel('Words', fontsize = 15)\n",
    "plt.ylabel('Frequency', fontsize = 15)\n",
    "plt.title(\"Top 20 Most Common Words for Programming Language 'C'\", fontsize = 20)\n",
    "sns.barplot(x=c_freq.index, y=c_freq.frequency, palette ='Purples_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:51:13.684463Z",
     "start_time": "2022-09-06T21:51:13.496670Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Vis for typescript\n",
    "# Storing frequency into dataframe\n",
    "type_freq = pd.DataFrame(pd.Series(typescript.split()).value_counts().head(20), columns = ['frequency'])\n",
    "# Visualize most frequent words for text repos\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.xlabel('Words', fontsize = 15)\n",
    "plt.ylabel('Frequency', fontsize = 15)\n",
    "plt.title(\"Top 20 Most Common Words for Programming Language 'TypeScript'\", fontsize = 20)\n",
    "sns.barplot(x=type_freq.index, y=type_freq.frequency, palette ='Purples_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div class=\"alert alert-info\">Takeaway: Yes, most common words are unique for each programming language. But there are a few overlap: 'project', 'app', etc.</div></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Are there any words that uniquely identify a programming language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:56:17.434798Z",
     "start_time": "2022-09-06T21:56:16.947024Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "text_bigrams = pd.Series(nltk.ngrams(text.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in text_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:55:38.868364Z",
     "start_time": "2022-09-06T21:55:38.860744Z"
    }
   },
   "source": [
    "#### üî∏\tJava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:56:06.569035Z",
     "start_time": "2022-09-06T21:56:06.132014Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "java_bigrams = pd.Series(nltk.ngrams(java.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in java_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:56:37.298348Z",
     "start_time": "2022-09-06T21:56:37.287526Z"
    }
   },
   "source": [
    "#### üî∏ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:57:00.326207Z",
     "start_time": "2022-09-06T21:56:59.886413Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "c_bigrams = pd.Series(nltk.ngrams(c.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in c_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî∏\tTypeScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:57:41.235489Z",
     "start_time": "2022-09-06T21:57:40.772030Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "typescript_bigrams = pd.Series(nltk.ngrams(typescript.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in typescript_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî∏ HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:58:27.869196Z",
     "start_time": "2022-09-06T21:58:27.438799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "html_bigrams = pd.Series(nltk.ngrams(html.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in html_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî∏\tother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:59:04.247208Z",
     "start_time": "2022-09-06T21:59:03.816879Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "other_bigrams = pd.Series(nltk.ngrams(other.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in other_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî∏\tPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:00:54.383372Z",
     "start_time": "2022-09-06T22:00:53.607440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "python_bigrams = pd.Series(nltk.ngrams(python.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in python_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî∏ Solidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:01:15.787499Z",
     "start_time": "2022-09-06T22:01:15.317834Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "solidity_bigrams = pd.Series(nltk.ngrams(solidity.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in solidity_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî∏\tCSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:01:41.665241Z",
     "start_time": "2022-09-06T22:01:41.212920Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Storing frequency into series\n",
    "css_bigrams = pd.Series(nltk.ngrams(css.split(),2)).value_counts()\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in css_bigrams.to_dict().items()}\n",
    "# Using wordcloud to visualize most common words\n",
    "img = WordCloud(background_color='white', width=800, height=400, colormap = 'Purples_r').generate_from_frequencies(data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:01:58.125533Z",
     "start_time": "2022-09-06T22:01:58.112836Z"
    }
   },
   "source": [
    "<h3><div class=\"alert alert-info\">Takeaway: Yes, there are bigrams that uniquely identify a programming language.</div></h3/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:02:17.734177Z",
     "start_time": "2022-09-06T22:02:17.729635Z"
    }
   },
   "source": [
    "<h1 align=\"center\">------------ üîÜ Modeling ------------</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:04:46.346140Z",
     "start_time": "2022-09-06T22:04:43.745617Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:04:52.247654Z",
     "start_time": "2022-09-06T22:04:51.850994Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Modeling environment setup\n",
    "# Data hanlding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Essential libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Data visualization\n",
    "import plotly\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# XGBoost classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:05:34.755063Z",
     "start_time": "2022-09-06T22:05:34.328932Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Fit and transform data for analysis\n",
    "# CVR Fit transform\n",
    "cvr = CountVectorizer(max_features = 55, ngram_range=(1, 3))\n",
    "words_train = cvr.fit_transform(train['readme_contents'])\n",
    "X_train_a = pd.DataFrame(words_train.todense(), columns=cvr.get_feature_names_out())\n",
    "# validate\n",
    "words_validate = cvr.transform(validate['readme_contents'])\n",
    "X_validate_a = pd.DataFrame(words_validate.todense(), columns=cvr.get_feature_names_out())\n",
    "# Built Test\n",
    "words_test = cvr.transform(test['readme_contents'])\n",
    "X_test_a= pd.DataFrame(words_test.todense(), columns=cvr.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:10:31.429701Z",
     "start_time": "2022-09-06T22:10:30.927243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 55, ngram_range=(1, 3))\n",
    "words_train = tfidf.fit_transform(train['readme_contents'])\n",
    "X_train_b = pd.DataFrame(words_train.todense(), columns=tfidf.get_feature_names_out())\n",
    "words_validate = tfidf.transform(validate['readme_contents'])\n",
    "X_validate_b = pd.DataFrame(words_validate.todense(), columns=tfidf.get_feature_names_out())\n",
    "words_test = tfidf.transform(test['readme_contents'])\n",
    "X_test_b = pd.DataFrame(words_test.todense(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:10:45.467706Z",
     "start_time": "2022-09-06T22:10:45.462529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build Target\n",
    "y_train = train.language\n",
    "y_validate = validate.language\n",
    "y_test = test.language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "- Decision Tree\n",
    "- SVM (Support vector machine) classifier\n",
    "- KNN (k-nearest neighbors) classifier \n",
    "- Naive Bayes classifier\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:11:40.414058Z",
     "start_time": "2022-09-06T22:11:40.383947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline\n",
    "y_train.mode()\n",
    "train['baseline_pred'] = 'text'\n",
    "baseline_accuracy = (train.language == train.baseline_pred).mean()\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:12:13.646703Z",
     "start_time": "2022-09-06T22:12:13.623382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validate Baseline\n",
    "validate['baseline_pred'] = 'text'\n",
    "baseline_accuracy = (validate.language == validate.baseline_pred).mean()\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree - Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:13:08.512018Z",
     "start_time": "2022-09-06T22:13:07.986136Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# max depth vs. model score, comparing training & validate datasets\n",
    "metrics = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    # Make the model\n",
    "    tree = DecisionTreeClassifier(max_depth=i, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    tree = tree.fit(X_train_a, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = tree.score(X_train_a, y_train)\n",
    "    \n",
    "    out_of_sample_accuracy = tree.score(X_validate_a, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"max_depth\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df.sort_values(by = ['validate_accuracy', 'difference'], ascending = [False, True])\n",
    "\n",
    "# Visualizing model performance as we change the max depth, check if there's overfitting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.max_depth, df.train_accuracy, marker = 'o', color = 'blue')\n",
    "plt.plot(df.max_depth, df.validate_accuracy, marker = 'o', color = 'red')\n",
    "plt.title('Overfitting Occurs at Higher Values for Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:13:35.550491Z",
     "start_time": "2022-09-06T22:13:33.626667Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create the tree\n",
    "tree = DecisionTreeClassifier(max_depth=8, random_state=123)\n",
    "\n",
    "# Fit the model on train\n",
    "tree = tree.fit(X_train_a, y_train)\n",
    "\n",
    "# Use the model\n",
    "# We'll evaluate the model's performance on train, first\n",
    "y_predictions = tree.predict(X_train_a)\n",
    "\n",
    "# Visualizing the tree\n",
    "fig, ax = plt.subplots(figsize=(12,6), dpi = 300)\n",
    "plot_tree(tree, feature_names=X_train_a.columns, class_names=y_train.unique(), filled=True, fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:14:53.039227Z",
     "start_time": "2022-09-06T22:14:53.014975Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.3f}'\n",
    "      .format(tree.score(X_train_a, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on validate set: {:.3f}'\n",
    "      .format(tree.score(X_validate_a, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:15:26.577389Z",
     "start_time": "2022-09-06T22:15:26.326462Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# max depth vs. model score, comparing training & validate datasets\n",
    "metrics = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    # Make the model\n",
    "    tree = DecisionTreeClassifier(max_depth=i, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    tree = tree.fit(X_train_b, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = tree.score(X_train_b, y_train)\n",
    "    \n",
    "    out_of_sample_accuracy = tree.score(X_validate_b, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"max_depth\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df.sort_values(by = ['validate_accuracy', 'difference'], ascending = [False, True])\n",
    "\n",
    "# Visualizing model performance as we change the max depth, check if there's overfitting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.max_depth, df.train_accuracy, marker = 'o', color = 'blue')\n",
    "plt.plot(df.max_depth, df.validate_accuracy, marker = 'o', color = 'red')\n",
    "plt.title('Overfitting Occurs at Higher Values for Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:15:52.809418Z",
     "start_time": "2022-09-06T22:15:52.778326Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Stat Info\n",
    "# Create the tree\n",
    "tree = DecisionTreeClassifier(max_depth=6, random_state=123)\n",
    "\n",
    "# Fit the model on train\n",
    "tree = tree.fit(X_train_b, y_train)\n",
    "\n",
    "# Use the model\n",
    "# We'll evaluate the model's performance on train, first\n",
    "y_predictions = tree.predict(X_train_b)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.3f}'\n",
    "      .format(tree.score(X_train_b, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on validate set: {:.3f}'\n",
    "      .format(tree.score(X_validate_b, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (Support Vector Machine) Classifier - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:17:25.788253Z",
     "start_time": "2022-09-06T22:17:25.666180Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# training a linear SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 2).fit(X_train_a, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_validate_a)\n",
    "\n",
    "# model accuracy for train and validate\n",
    "accuracy = svm_model_linear.score(X_validate_a, y_validate)\n",
    "print('Accuracy of SVM classifier on training set: {:.3f}'\n",
    "      .format(svm_model_linear.score(X_train_a, y_train)))\n",
    "print('Accuracy of SVM classifier on validate set: {:.3f}'\n",
    "      .format(svm_model_linear.score(X_validate_a, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (Support Vector Machine) Classifier - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:17:48.631936Z",
     "start_time": "2022-09-06T22:17:48.547564Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# training a linear SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear', C =3).fit(X_train_b, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_validate_b)\n",
    "\n",
    "# model accuracy for train and validate\n",
    "accuracy = svm_model_linear.score(X_validate_a, y_validate)\n",
    "print('Accuracy of SVM classifier on training set: {:.3f}'\n",
    "      .format(svm_model_linear.score(X_train_b, y_train)))\n",
    "print('Accuracy of SVM classifier on validate set: {:.3f}'\n",
    "      .format(svm_model_linear.score(X_validate_b, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:18:29.872507Z",
     "start_time": "2022-09-06T22:18:29.773119Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# training a KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 4).fit(X_train_a, y_train)\n",
    "\n",
    "# model accuracy for train and validate \n",
    "print('Accuracy of SVM classifier on training set: {:.3f}'\n",
    "      .format(knn.score(X_train_a, y_train)))\n",
    "print('Accuracy of SVM classifier on validate set: {:.3f}'\n",
    "      .format(knn.score(X_validate_a, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:19:40.389055Z",
     "start_time": "2022-09-06T22:19:40.163491Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# training a KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5).fit(X_train_b, y_train)\n",
    "\n",
    "# model accuracy for train and validate \n",
    "print('Accuracy of SVM classifier on training set: {:.3f}'\n",
    "      .format(knn.score(X_train_b, y_train)))\n",
    "print('Accuracy of SVM classifier on validate set: {:.3f}'\n",
    "      .format(knn.score(X_validate_b, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:20:08.633203Z",
     "start_time": "2022-09-06T22:20:08.585185Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# training a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(X_train_a, y_train)\n",
    "\n",
    "# model accuracy for X_test  \n",
    "print('Accuracy of SVM classifier on training set: {:.3f}'\n",
    "      .format(gnb.score(X_train_a, y_train)))\n",
    "print('Accuracy of SVM classifier on validate set: {:.3f}'\n",
    "      .format(gnb.score(X_validate_a, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:20:43.058314Z",
     "start_time": "2022-09-06T22:20:43.016790Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# training a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(X_train_b, y_train)\n",
    "\n",
    "# model accuracy for X_test  \n",
    "print('Accuracy of SVM classifier on training set: {:.3f}'\n",
    "      .format(gnb.score(X_train_b, y_train)))\n",
    "print('Accuracy of SVM classifier on validate set: {:.3f}'\n",
    "      .format(gnb.score(X_validate_b, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:21:12.945090Z",
     "start_time": "2022-09-06T22:21:12.930868Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#XGBoost magic\n",
    "from xgboost import XGBClassifier\n",
    "def xg_model(x_train, y_train, x_validate, y_validate, x_test, y_test):\n",
    "\n",
    "    # Fit model to training data\n",
    "    model = XGBClassifier()\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "\n",
    "    y_v_pred = model.predict(x_validate)\n",
    "\n",
    "    y_t_pred = model.predict(x_test)\n",
    "    print(classification_report(y_train, y_pred))\n",
    "\n",
    "    print(classification_report(y_validate, y_v_pred))\n",
    "    \n",
    "    print(classification_report(y_test, y_t_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:21:49.446706Z",
     "start_time": "2022-09-06T22:21:49.421949Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# New XG Boost dictionary\n",
    "new_y_train = y_train.map({'C': 0, 'CSS': 1, 'Go': 2, 'HTML': 3, 'Java':4, 'Python': 5, 'Rust': 6, 'Solidity': 7, 'TypeScript': 8, 'other': 9, 'text':10})\n",
    "new_y_validate = y_validate.map({'C': 0, 'CSS': 1, 'Go': 2, 'HTML': 3, 'Java':4, 'Python': 5, 'Rust': 6, 'Solidity': 7, 'TypeScript': 8, 'other': 9, 'text':10})\n",
    "new_y_test = y_test.map({'C': 0, 'CSS': 1, 'Go': 2, 'HTML': 3, 'Java':4, 'Python': 5, 'Rust': 6, 'Solidity': 7, 'TypeScript': 8, 'other': 9, 'text':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:23:18.049876Z",
     "start_time": "2022-09-06T22:23:17.605891Z"
    }
   },
   "outputs": [],
   "source": [
    "xg_model(X_train_a, new_y_train, X_validate_a, new_y_validate, X_test_a, new_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T22:23:15.087958Z",
     "start_time": "2022-09-06T22:23:14.645193Z"
    }
   },
   "outputs": [],
   "source": [
    "xg_model(X_train_b, new_y_train, X_validate_b, new_y_validate, X_test_b, new_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">------------ üîÜ Conclusion ------------</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I don't understand the models... it's making my brain itch. \n",
    "- We did the thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
